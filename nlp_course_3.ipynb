{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') # load the english language library (load a model called nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN poss\n",
      "'s PART case\n",
      "stock NOUN nsubj\n",
      "is VERB aux\n",
      "dropping VERB ROOT\n",
      ". PUNCT punct\n",
      "Oh INTJ intj\n",
      "the DET det\n",
      "horror NOUN ROOT\n",
      "! PUNCT punct\n",
      "Tesla's stock is dropping.\n",
      "Oh the horror!\n"
     ]
    }
   ],
   "source": [
    "def tokenize(doc):\n",
    "    for token in doc:\n",
    "        print(token.text,token.pos_,token.dep_) # token attributes (e.g. the part of speech)\n",
    "\n",
    "# reads in the unicode string and parses the sentence into tokens (individual words)\n",
    "doc = nlp(u'My name is Brandon and I have to write a card for my birthday.') # doc holds the processed text\n",
    "# nlp() -> the text enters a \"processing pipeline,\" which breaks down the text and conducts a series of operations on it\n",
    "\n",
    "# tokenization:\n",
    "doc2 = nlp(u\"Tesla's stock is dropping. Oh the horror!\")\n",
    "tokenize(doc2)\n",
    "# can access specific tokens in the doc object with indexing (doc2[0])\n",
    "\n",
    "# we can slice the processed string and take certain tokens (e.g. doc[0:3])\n",
    "# spacy will convert this to a Span object because we are taking a range of the tokens\n",
    "\n",
    "# spacy can automatically seperate the string based on sentences\n",
    "for sentence in doc2.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization is breaking up the sentence into smaller components (e.g. words)\n",
    "# this is the fundamental process for understanding the sentence\n",
    "# we split by whitespace, then remove characters in the beginning/end, "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf2ab4710bb88b95cde5549200cc7126844ad4b46382c8d084742582274cbd47"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('nlp_course': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
